'''
ä»£ç åç§°ï¼šçˆ¬å–äººæ°‘æ—¥æŠ¥æ•°æ®ä¸ºtxtæ–‡ä»¶ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
ç¼–å†™æ—¥æœŸï¼š2025å¹´1æœˆ1æ—¥
ä½œè€…ï¼šgithubï¼ˆcaspiankexinï¼‰ä¿®æ”¹å¢å¼ºç‰ˆ
ç‰ˆæœ¬ï¼šç¬¬4ç‰ˆ
åŠŸèƒ½ï¼šçˆ¬å–äººæ°‘æ—¥æŠ¥æ–°é—»æ•°æ®å¹¶æ˜¾ç¤ºè¿›åº¦æ¡
å¯çˆ¬å–çš„æ—¶é—´èŒƒå›´ï¼š2024å¹´12æœˆèµ·
æ³¨æ„ï¼šæ­¤ä»£ç ä»…ä¾›äº¤æµå­¦ä¹ ï¼Œä¸å¾—ç”¨äºå…¶ä»–ç”¨é€”ã€‚
'''

import requests
import bs4
import os
import datetime
import time
from tqdm import tqdm   # æ–°å¢è¿›åº¦æ¡æ¨¡å—

def fetchUrl(url):
    '''
    åŠŸèƒ½ï¼šè®¿é—® url çš„ç½‘é¡µï¼Œè·å–ç½‘é¡µå†…å®¹å¹¶è¿”å›
    å‚æ•°ï¼šç›®æ ‡ç½‘é¡µçš„ url
    è¿”å›ï¼šç›®æ ‡ç½‘é¡µçš„ html å†…å®¹
    '''
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
    }

    r = requests.get(url, headers=headers)
    r.raise_for_status()
    r.encoding = r.apparent_encoding
    return r.text


def getPageList(year, month, day):
    '''
    åŠŸèƒ½ï¼šè·å–å½“å¤©æŠ¥çº¸çš„å„ç‰ˆé¢çš„é“¾æ¥åˆ—è¡¨
    å‚æ•°ï¼šå¹´ï¼Œæœˆï¼Œæ—¥
    '''
    url = f'http://paper.people.com.cn/rmrb/pc/layout/{year}{month}/{day}/node_01.html'
    html = fetchUrl(url)
    bsobj = bs4.BeautifulSoup(html, 'html.parser')
    temp = bsobj.find('div', attrs={'id': 'pageList'})
    if temp:
        pageList = temp.ul.find_all('div', attrs={'class': 'right_title-name'})
    else:
        pageList = bsobj.find('div', attrs={'class': 'swiper-container'}).find_all('div', attrs={'class': 'swiper-slide'})
    linkList = []

    for page in pageList:
        link = page.a["href"]
        url = f'http://paper.people.com.cn/rmrb/pc/layout/{year}{month}/{day}/{link}'
        linkList.append(url)

    return linkList


def getTitleList(year, month, day, pageUrl):
    '''
    åŠŸèƒ½ï¼šè·å–æŠ¥çº¸æŸä¸€ç‰ˆé¢çš„æ–‡ç« é“¾æ¥åˆ—è¡¨
    å‚æ•°ï¼šå¹´ï¼Œæœˆï¼Œæ—¥ï¼Œè¯¥ç‰ˆé¢çš„é“¾æ¥
    '''
    html = fetchUrl(pageUrl)
    bsobj = bs4.BeautifulSoup(html, 'html.parser')
    temp = bsobj.find('div', attrs={'id': 'titleList'})
    if temp:
        titleList = temp.ul.find_all('li')
    else:
        titleList = bsobj.find('ul', attrs={'class': 'news-list'}).find_all('li')
    linkList = []

    for title in titleList:
        tempList = title.find_all('a')
        for temp in tempList:
            link = temp["href"]
            if 'content' in link:
                url = f'http://paper.people.com.cn/rmrb/pc/content/{year}{month}/{day}/{link}'
                linkList.append(url)

    return linkList


def getContent(html):
    '''
    åŠŸèƒ½ï¼šè§£æ HTML ç½‘é¡µï¼Œè·å–æ–°é—»çš„æ–‡ç« å†…å®¹
    å‚æ•°ï¼šhtml ç½‘é¡µå†…å®¹
    '''
    bsobj = bs4.BeautifulSoup(html, 'html.parser')

    # è·å–æ–‡ç« æ ‡é¢˜
    try:
        title = bsobj.h3.text + '\n' + bsobj.h1.text + '\n' + bsobj.h2.text + '\n'
    except:
        title = 'ï¼ˆæ— æ ‡é¢˜ï¼‰\n'

    # è·å–æ–‡ç« å†…å®¹
    pList = bsobj.find('div', attrs={'id': 'ozoom'}).find_all('p')
    content = ''
    for p in pList:
        content += p.text + '\n'

    return title + content


def saveFile(content, path, filename):
    '''
    åŠŸèƒ½ï¼šå°†æ–‡ç« å†…å®¹ content ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ä¸­
    å‚æ•°ï¼šè¦ä¿å­˜çš„å†…å®¹ï¼Œè·¯å¾„ï¼Œæ–‡ä»¶å
    '''
    if not os.path.exists(path):
        os.makedirs(path)

    file_path = os.path.join(path, filename)
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)


def download_rmrb(year, month, day, destdir):
    '''
    åŠŸèƒ½ï¼šçˆ¬å–ã€Šäººæ°‘æ—¥æŠ¥ã€‹ç½‘ç«™ æŸå¹´ æŸæœˆ æŸæ—¥ çš„æ–°é—»å†…å®¹ï¼Œå¹¶ä¿å­˜åœ¨ æŒ‡å®šç›®å½•ä¸‹
    å‚æ•°ï¼šå¹´ï¼Œæœˆï¼Œæ—¥ï¼Œæ–‡ä»¶ä¿å­˜çš„æ ¹ç›®å½•
    '''
    pageList = getPageList(year, month, day)
    print(f"\nå¼€å§‹çˆ¬å– {year}-{month}-{day} çš„æ•°æ®ï¼Œå…± {len(pageList)} ä¸ªç‰ˆé¢...\n")

    for pageNo, page in enumerate(tqdm(pageList, desc=f"{year}-{month}-{day} ç‰ˆé¢è¿›åº¦", ncols=80), start=1):
        try:
            titleList = getTitleList(year, month, day, page)
            for titleNo, url in enumerate(tqdm(titleList, desc=f"ç¬¬{pageNo}ç‰ˆæ–‡ç« è¿›åº¦", ncols=80, leave=False), start=1):
                html = fetchUrl(url)
                content = getContent(html)
                path = os.path.join(destdir, f"{year}{month}{day}")
                fileName = f"{year}{month}{day}-{str(pageNo).zfill(2)}-{str(titleNo).zfill(2)}.txt"
                saveFile(content, path, fileName)
        except Exception as e:
            print(f"æ—¥æœŸ {year}-{month}-{day} ä¸‹çš„ç‰ˆé¢ {page} å‡ºç°é”™è¯¯ï¼š{e}")
            continue


def gen_dates(b_date, days):
    day = datetime.timedelta(days=1)
    for i in range(days):
        yield b_date + day * i


def get_date_list(beginDate, endDate):
    '''
    è·å–æ—¥æœŸåˆ—è¡¨
    '''
    start = datetime.datetime.strptime(beginDate, "%Y%m%d")
    end = datetime.datetime.strptime(endDate, "%Y%m%d")

    data = []
    for d in gen_dates(start, (end - start).days + 1):
        data.append(d)
    return data


if __name__ == '__main__':
    print("æ¬¢è¿ä½¿ç”¨äººæ°‘æ—¥æŠ¥çˆ¬è™«ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰ï¼")
    beginDate = input('è¯·è¾“å…¥å¼€å§‹æ—¥æœŸï¼ˆå¦‚20241201ï¼‰:')
    endDate = input('è¯·è¾“å…¥ç»“æŸæ—¥æœŸï¼ˆå¦‚20241205ï¼‰:')
    destdir = input("è¯·è¾“å…¥æ•°æ®ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¦‚ D:/rmrb_dataï¼‰:")

    data = get_date_list(beginDate, endDate)
    print(f"\nå°†çˆ¬å– {len(data)} å¤©çš„äººæ°‘æ—¥æŠ¥æ•°æ®...\n")

    for d in tqdm(data, desc="æ•´ä½“è¿›åº¦ï¼ˆæŒ‰å¤©ï¼‰", ncols=80):
        year = str(d.year)
        month = str(d.month).zfill(2)
        day = str(d.day).zfill(2)
        download_rmrb(year, month, day, destdir)
        print(f"âœ… {year}-{month}-{day} çˆ¬å–å®Œæˆï¼")
        time.sleep(5)  # é˜²æ­¢è¢«å°IPï¼Œå¯è°ƒæ•´

    input("\nğŸ‰ æ‰€æœ‰æ—¥æœŸçˆ¬å–å®Œæˆï¼æŒ‰å›è½¦é”®é€€å‡ºç¨‹åºã€‚")
